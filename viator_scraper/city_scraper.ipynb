{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This module contains functions to scrape all the activities listed on a city page.\"\"\"\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_url_list(city_url, max_pages=100):\n",
    "    \"\"\"The city url is the first page of activities, most city urls have multiple pages\n",
    "    of activities, so we need to check if the page exists and if it does, add it to\n",
    "    a dictionary of valid pages. The key is the page number and the value is the url.\n",
    "    The variable max_pages is the maximum number of pages to check, this is to prevent\n",
    "    the scraper from running for too long if the city has a lot of pages.\"\"\"\n",
    "\n",
    "    response = requests.get(city_url, timeout=5, allow_redirects=False)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"City url is not valid: {city_url}\")\n",
    "        return {}\n",
    "    print(f\"Checked validity for page 1 and it was 200\")\n",
    "\n",
    "    valid_pages = {1: response}\n",
    "    if max_pages > 1:\n",
    "        for page_number in range(2, max_pages+1):\n",
    "            start = time.time()\n",
    "            page_url = city_url + f\"/{page_number}\"\n",
    "            response = requests.get(page_url, timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                valid_pages[page_number] = response\n",
    "            print(f\"Checked validity for page {page_number} and it was {response.status_code}. Returned in {time.time() - start} seconds\")\n",
    "    print(f\"Found {len(valid_pages)} valid pages\")\n",
    "    return valid_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activity_urls(soup):\n",
    "    \"\"\"This function takes bs4 soup object of a city activity page and returns a list of\n",
    "    activity urls contained in it.\"\"\"\n",
    "\n",
    "    activity_urls = []\n",
    "    activity_links = soup.find_all(\"a\", class_='text-dark highlight-able card-link')\n",
    "    for link in activity_links:\n",
    "        activity_urls.append(\"https://viator.com\" + link['href'])\n",
    "    return activity_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(link):\n",
    "    \"\"\"This function takes a link to an activity page and returns a list\n",
    "    of the data contained in it.\"\"\"\n",
    "\n",
    "    data = []\n",
    "    response = requests.get(link, timeout=5)\n",
    "    soup = bs(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "    activity_name = soup.find(\"h1\", class_=\"title__1Wwg title2__C3R7\").text\n",
    "    activity_url = link\n",
    "    price_per_participant = soup.find_all(\"span\", class_=\"moneyView__2HPx defaultColor__1NL9\")[0].text\n",
    "    valid_number_of_participants = soup.find(\"input\", class_ = \"input__MNXR md__1Fp3\")['data-automation-value']\n",
    "    valid_date = soup.find(\"input\", class_ = \"input__2pmO md__2bZz\")[\"value\"]\n",
    "\n",
    "    overview_block = soup.find(\"div\", class_=\"overviewWrapper__bMs4\")\n",
    "\n",
    "    main_overview_ = overview_block.find_all(\"div\")[0]\n",
    "    try:\n",
    "        activity_description = main_overview_.find_all(\"div\")[0].text\n",
    "    except:\n",
    "        activity_description = \"No description available\"\n",
    "\n",
    "    duration_block = soup.find_all(\"div\", class_ = \"item__3eVq\")[1]\n",
    "    duration = duration_block.find_all(\"div\")[1].text\n",
    "\n",
    "    avg_rating = soup.find(\"span\", class_ = \"averageRatingValue__Q1ep\").text\n",
    "\n",
    "    review_count = soup.find(\"div\", class_ = \"reviewCount__3sJa\").text\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    data.append(activity_name)\n",
    "    data.append(activity_url)\n",
    "    data.append(price_per_participant)\n",
    "    data.append(valid_number_of_participants)\n",
    "    data.append(valid_date)\n",
    "    data.append(activity_description)\n",
    "    data.append(duration)\n",
    "    data.append(avg_rating)\n",
    "    data.append(review_count)\n",
    "\n",
    "    return data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_scraper(city_url, max_pages=100, dataframe=pd.DataFrame(columns=['Activity Name', 'Activity URL', 'Price per Participant', 'Valid Number of Participants', 'Valid Date', 'Activity Description', 'Duration', 'Average Rating', 'No. of Reviews'])):\n",
    "    \"\"\"This function takes a city url and returns a dataframe of all the activities\n",
    "    listed on the city page.\"\"\"\n",
    "\n",
    "    valid_pages = get_city_url_list(city_url, max_pages)\n",
    "    activity_urls = []\n",
    "    for page_number, response in valid_pages.items():\n",
    "        start = time.time()\n",
    "        soup = bs(response.text, 'html.parser')\n",
    "        activity_urls += get_activity_urls(soup)\n",
    "        print(f\"Activity urls for page {page_number} added to list in {time.time() - start} seconds\")\n",
    "    print(f\"Found {len(activity_urls)} activity urls\")\n",
    "\n",
    "    print(f\"Estimated time to completion: {len(activity_urls) * 1.8} seconds\")\n",
    "\n",
    "    activity_len = len(activity_urls)\n",
    "    for link in activity_urls:\n",
    "        start = time.time()\n",
    "        try:\n",
    "            data = extract_data(link)\n",
    "            dataframe.loc[len(dataframe)] = data\n",
    "            activity_len -= 1\n",
    "            print(f\"Extracted data for {link} in {time.time() - start} seconds. {activity_len} activities left\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting data for {link} because {e}\")\n",
    "            continue\n",
    "    print(f\"Finished scraping {city_url}\")\n",
    "    return dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked validity for page 1 and it was 200\n",
      "Checked validity for page 2 and it was 200. Returned in 1.0540146827697754 seconds\n",
      "Checked validity for page 3 and it was 200. Returned in 0.8984968662261963 seconds\n",
      "Checked validity for page 4 and it was 200. Returned in 0.9331660270690918 seconds\n",
      "Checked validity for page 5 and it was 200. Returned in 1.4653961658477783 seconds\n",
      "Checked validity for page 6 and it was 200. Returned in 1.0260207653045654 seconds\n",
      "Checked validity for page 7 and it was 200. Returned in 0.905811071395874 seconds\n",
      "Checked validity for page 8 and it was 200. Returned in 1.3069498538970947 seconds\n",
      "Checked validity for page 9 and it was 200. Returned in 1.1144258975982666 seconds\n",
      "Checked validity for page 10 and it was 200. Returned in 1.2990829944610596 seconds\n",
      "Checked validity for page 11 and it was 200. Returned in 0.9520149230957031 seconds\n",
      "Checked validity for page 12 and it was 200. Returned in 1.1556456089019775 seconds\n",
      "Checked validity for page 13 and it was 200. Returned in 0.8081591129302979 seconds\n",
      "Checked validity for page 14 and it was 200. Returned in 1.4134352207183838 seconds\n",
      "Checked validity for page 15 and it was 200. Returned in 1.2531976699829102 seconds\n",
      "Found 15 valid pages\n",
      "Activity urls for page 1 added to list in 0.4133262634277344 seconds\n",
      "Activity urls for page 2 added to list in 0.5338523387908936 seconds\n",
      "Activity urls for page 3 added to list in 0.44737672805786133 seconds\n",
      "Activity urls for page 4 added to list in 0.371868371963501 seconds\n",
      "Activity urls for page 5 added to list in 0.5160727500915527 seconds\n",
      "Activity urls for page 6 added to list in 0.4770524501800537 seconds\n",
      "Activity urls for page 7 added to list in 0.3306889533996582 seconds\n",
      "Activity urls for page 8 added to list in 0.49700260162353516 seconds\n",
      "Activity urls for page 9 added to list in 0.8297207355499268 seconds\n",
      "Activity urls for page 10 added to list in 0.5368199348449707 seconds\n",
      "Activity urls for page 11 added to list in 0.48679614067077637 seconds\n",
      "Activity urls for page 12 added to list in 0.35770535469055176 seconds\n",
      "Activity urls for page 13 added to list in 0.32623767852783203 seconds\n",
      "Activity urls for page 14 added to list in 0.3930234909057617 seconds\n",
      "Activity urls for page 15 added to list in 0.22109413146972656 seconds\n",
      "Found 344 activity urls\n",
      "Estimated time to completion: 619.2 seconds\n",
      "Extracted data for https://viator.com/tours/Phnom-Penh/Excursion-Phnom-Penh-Full-Days-Private-Tours/d5425-103195P21 in 2.243847370147705 seconds. 343 activities left\n",
      "Extracted data for https://viator.com/tours/Phnom-Penh/Sunset-Cruise-with-unlimited-beer-and-soft-drinks-English-speaking-guide-on-board/d5425-92485P30 in 1.9485864639282227 seconds. 342 activities left\n",
      "Extracted data for https://viator.com/tours/Phnom-Penh/Killing-Field-And-Genocide-Museum-Tour/d5425-147415P2 in 2.171081304550171 seconds. 341 activities left\n",
      "Extracted data for https://viator.com/tours/Phnom-Penh/Private-overland-transfer-from-Phnom-Penh-to-Siem-Reap/d5425-39527P61 in 2.0881567001342773 seconds. 340 activities left\n",
      "Extracted data for https://viator.com/tours/Phnom-Penh/Private-3-Day-Mekong-Delta-River-Tour-from-Phnom-Penh-to-Ho-Chi-Minh-City/d5425-6730PNHVNT_SGN04 in 1.794473648071289 seconds. 339 activities left\n",
      "Extracted data for https://viator.com/tours/Phnom-Penh/Phnom-Penh-Vital-Discovery-Full-Day-Tour-Including-all-services/d5425-145577P22 in 1.646559476852417 seconds. 338 activities left\n",
      "Extracted data for https://viator.com/tours/Phnom-Penh/Evening-Market-and-BBQ-Tour/d5425-87335P1 in 2.1555325984954834 seconds. 337 activities left\n",
      "Extracted data for https://viator.com/tours/Phnom-Penh/3-Days-Phnom-Penh-and-Siem-Reap/d5425-168645P21 in 1.7600443363189697 seconds. 336 activities left\n",
      "Extracted data for https://viator.com/tours/Phnom-Penh/Silk-Island-Biking-Tour/d5425-75526P1 in 2.0444436073303223 seconds. 335 activities left\n",
      "Extracted data for https://viator.com/tours/Phnom-Penh/Killing-field-and-Genocide-museum-Tour/d5425-182749P1 in 2.1044704914093018 seconds. 334 activities left\n",
      "Extracted data for https://viator.com/tours/Phnom-Penh/Phnom-Penh-Evening-Food-and-Craft-Beer-Tour-by-Tuktuk/d5425-7494P11 in 1.3993210792541504 seconds. 333 activities left\n",
      "Extracted data for https://viator.com/tours/Phnom-Penh/Oudong-and-Koh-Chen-Island-Full-Day-Cruise-and-Tour-with-English-speaking-guide/d5425-92485P28 in 2.1642603874206543 seconds. 332 activities left\n",
      "Extracted data for https://viator.com/tours/Phnom-Penh/Private-Car-Transfer-from-Phnom-Penh-Battambang-Phnom-Penh/d5425-39527P91 in 1.5571177005767822 seconds. 331 activities left\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = pd.DataFrame(columns=['Activity Name', 'Activity URL', 'Price per Participant', 'Valid Number of Participants', 'Valid Date', 'Activity Description', 'Duration', 'Average Rating', 'No. of Reviews'])\n",
    "    df = main_scraper(\"https://www.viator.com/Bangkok/d343-ttd\", max_pages = 84)\n",
    "    df.to_excel(\"bangkok_activities.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "viator-scraper-6I9V9I7P-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "737046e8c5d5e159bfec73f9acea253898ef59efb19a20c4db1d048b41812d63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
