{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This module contains functions to scrape all the activities listed on a city page.\"\"\"\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_url_list(city_url, max_pages=100):\n",
    "    \"\"\"The city url is the first page of activities, most city urls have multiple pages\n",
    "    of activities, so we need to check if the page exists and if it does, add it to\n",
    "    a dictionary of valid pages. The key is the page number and the value is the url.\n",
    "    The variable max_pages is the maximum number of pages to check, this is to prevent\n",
    "    the scraper from running for too long if the city has a lot of pages.\"\"\"\n",
    "\n",
    "    response = requests.get(city_url, timeout=5, allow_redirects=False)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"City url is not valid: {city_url}\")\n",
    "        return {}\n",
    "    print(f\"Checked validity for page 1 and it was 200\")\n",
    "\n",
    "    valid_pages = {1: response}\n",
    "    if max_pages > 1:\n",
    "        for page_number in range(2, max_pages+1):\n",
    "            start = time.time()\n",
    "            page_url = city_url + f\"/{page_number}\"\n",
    "            response = requests.get(page_url, timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                valid_pages[page_number] = response\n",
    "            print(f\"Checked validity for page {page_number} and it was {response.status_code}. Returned in {time.time() - start} seconds\")\n",
    "    print(f\"Found {len(valid_pages)} valid pages\")\n",
    "    return valid_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activity_urls(soup):\n",
    "    \"\"\"This function takes bs4 soup object of a city activity page and returns a list of\n",
    "    activity urls contained in it.\"\"\"\n",
    "\n",
    "    activity_urls = []\n",
    "    activity_links = soup.find_all(\"a\", class_='text-dark highlight-able card-link')\n",
    "    for link in activity_links:\n",
    "        activity_urls.append(\"https://viator.com\" + link['href'])\n",
    "    return activity_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(link):\n",
    "    \"\"\"This function takes a link to an activity page and returns a list\n",
    "    of the data contained in it.\"\"\"\n",
    "\n",
    "    data = []\n",
    "    response = requests.get(link, timeout=5)\n",
    "    soup = bs(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "    activity_name = soup.find(\"h1\", class_=\"title__1Wwg title2__C3R7\").text\n",
    "\n",
    "    activity_url = link\n",
    "\n",
    "    price_per_participant = soup.find_all(\"span\", class_=\"moneyView__2HPx defaultColor__1NL9\")[0].text\n",
    "\n",
    "    valid_number_of_participants = soup.find(\"input\", class_ = \"input__MNXR md__1Fp3\")['data-automation-value']\n",
    "\n",
    "    valid_date = soup.find(\"input\", class_ = \"input__2pmO md__2bZz\")[\"value\"]\n",
    "\n",
    "    try:\n",
    "      overview_block = soup.find(\"div\", class_=\"overviewWrapper__bMs4\")\n",
    "      main_overview = overview_block.find_all(\"div\")[0]\n",
    "      activity_description = main_overview.find_all(\"div\")[0].text\n",
    "    except:\n",
    "      activity_description = \"Timed out while waiting for description. Please check link for description.\"\n",
    "      print(activity_description)\n",
    "\n",
    "    details_block = soup.find_all(\"div\", class_ = \"item__3eVq\")\n",
    "    details = [x.find_all(\"div\")[1].text for x in details_block]\n",
    "    duration = \"\"\n",
    "    for detail in details:\n",
    "        if any(chr.isdigit() for chr in detail):\n",
    "          duration = detail\n",
    "\n",
    "    try:\n",
    "      avg_rating = soup.find(\"span\", class_ = \"averageRatingValue__Q1ep\").text\n",
    "      review_count = soup.find(\"div\", class_ = \"reviewCount__3sJa\").text\n",
    "    except:\n",
    "      avg_rating = \"No rating available\"\n",
    "      review_count = \"No review available\"\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    data.append(activity_name)\n",
    "    data.append(activity_url)\n",
    "    data.append(price_per_participant)\n",
    "    data.append(valid_number_of_participants)\n",
    "    data.append(valid_date)\n",
    "    data.append(activity_description)\n",
    "    data.append(duration)\n",
    "    data.append(avg_rating)\n",
    "    data.append(review_count)\n",
    "\n",
    "    return data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_scraper(city_url, max_pages=100, dataframe=pd.DataFrame(columns=['Activity Name', 'Activity URL', 'Price per Participant', 'Valid Number of Participants', 'Valid Date', 'Activity Description', 'Duration', 'Average Rating', 'No. of Reviews'])):\n",
    "    \"\"\"This function takes a city url and returns a dataframe of all the activities\n",
    "    listed on the city page.\"\"\"\n",
    "\n",
    "    valid_pages = get_city_url_list(city_url, max_pages)\n",
    "    activity_urls = []\n",
    "    for page_number, response in valid_pages.items():\n",
    "        start = time.time()\n",
    "        soup = bs(response.text, 'html.parser')\n",
    "        activity_urls += get_activity_urls(soup)\n",
    "        print(f\"Activity urls for page {page_number} added to list in {time.time() - start} seconds\")\n",
    "    print(f\"Found {len(activity_urls)} activity urls\")\n",
    "\n",
    "    print(f\"Estimated time to completion: {len(activity_urls) * 1.8} seconds\")\n",
    "\n",
    "    activity_len = len(activity_urls)\n",
    "    for link in activity_urls:\n",
    "        start = time.time()\n",
    "        try:\n",
    "            data = extract_data(link)\n",
    "            dataframe.loc[len(dataframe)] = data\n",
    "            activity_len -= 1\n",
    "            print(f\"Extracted data for {link} in {time.time() - start} seconds. {activity_len} activities left\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting data for {link} because {e}\")\n",
    "            continue\n",
    "    print(f\"Finished scraping {city_url}\")\n",
    "    return dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = pd.DataFrame(columns=['Activity Name', 'Activity URL', 'Price per Participant', 'Valid Number of Participants', 'Valid Date', 'Activity Description', 'Duration', 'Average Rating', 'No. of Reviews'])\n",
    "    df = main_scraper(\"https://www.viator.com/Bangkok/d343-ttd\", max_pages = 84)\n",
    "    df.to_excel(\"bangkok_activities.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "viator-scraper-6I9V9I7P-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "737046e8c5d5e159bfec73f9acea253898ef59efb19a20c4db1d048b41812d63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
